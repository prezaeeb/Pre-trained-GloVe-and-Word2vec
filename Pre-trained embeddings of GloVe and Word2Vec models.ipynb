{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Data\n",
    "train_data = pd.read_csv('Train_cleaned2.csv', error_bad_lines=False, delimiter= '\\t')\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('/Test_CLEAN2.csv', error_bad_lines=False, delimiter= '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GloveVectorizer:\n",
    "  def __init__(self):\n",
    "    # load in pre-trained word vectors\n",
    "    print('Loading word vectors...')\n",
    "    word2vec = {}\n",
    "    embedding = []\n",
    "    idx2word = []\n",
    "    #with open( '/home/parisa/programming/data/Pre_trained_data/glove/twitterGlove/glove.twitter.27B.200d.txt') as f:\n",
    "    with open('/home/parisa/nlp_class_dl/data/large_files/glove.6B/glove.6B.300d.txt') as f:\n",
    "      # is just a space-separated text file in the format:\n",
    "      # word vec[0] vec[1] vec[2] ...\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "    print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "    # save for later\n",
    "    self.word2vec = word2vec\n",
    "    self.embedding = np.array(embedding)\n",
    "    self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "    print(self.embedding.shape)\n",
    "    self.V, self.D = self.embedding.shape\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.lower().split()\n",
    "      vecs = []\n",
    "      for word in tokens:\n",
    "        if word in self.word2vec:\n",
    "          vec = self.word2vec[word]\n",
    "          vecs.append(vec)\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = GloveVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(Train_data['conversations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Word2VecVectorizer:\n",
    "  def __init__(self):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = KeyedVectors.load_word2vec_format(\n",
    "      '/home/parisa/nlp_class_dl/data/large_files/GoogleNews-vectors-negative300.bin',\n",
    "      binary=True\n",
    "    )\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('king')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Word2VecVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(Train_data['conversations'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
